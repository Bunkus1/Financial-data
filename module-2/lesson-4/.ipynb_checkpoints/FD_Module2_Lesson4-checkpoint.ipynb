{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FINANCIAL DATA\n",
    "MODULE 2 | LESSON 4\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# USING STATISTICAL DISTRIBUTIONS TO MODEL STOCK RETURNS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|  |  |\n",
    "|:---|:---|\n",
    "|**Reading Time** |  30 minutes |\n",
    "|**Prior Knowledge** | Statistics, Cumulative distribution functions |\n",
    "|**Keywords** | Normal Distribution, Student T distribution, Normality Testing, P-values |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We looked at various ways to measure risk and returns in the last lesson. Here, we take that a step further by using distributions to correctly model risk and returns.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_datareader.data as web\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. How Are Stock Returns Distributed?\n",
    "Many models and theories surrounding stocks assume a normal distribution. We will try to determine that here with a data-based analysis. Properties of a Gaussian distribution are as follows:\n",
    "\n",
    "* Mean, median, and mode are all the same\n",
    "* The data is symmetrical, meaning there are equal counts of observations on both sides of the mean.\n",
    "* In normally distributed data, 68.25% of all cases fall within +/- one standard deviation from the mean, 95% of all cases fall within +/- two standard deviations from the mean, and 99.7% of all cases fall within +/- three standard deviations from the mean.\n",
    "\n",
    "Let's start by pulling 20 years of daily price data for the S&P 500. We'll use similar methods we've used in the last few lessons to pull this data and will calculate the log returns here.\n",
    "\n",
    "One quick way of doing this is to determine how many data points we have on either side of the mean here. We have a bit more than 5,000 data points. The below code takes the count of data points greater than the mean and divides it by the total number of data points. This will give us the percentage of data points greater than the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.date.today() - datetime.timedelta(365 * 20)\n",
    "end = datetime.date.today()\n",
    "prices = web.DataReader([\"^GSPC\"], \"yahoo\", start, end)[\"Adj Close\"]\n",
    "\n",
    "# Rename column to make names more intuitive\n",
    "prices = prices.rename(columns={\"^GSPC\": \"SP500\"})\n",
    "df = np.log(prices) - np.log(prices.shift(1))\n",
    "df = df.iloc[1:, 0:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Are returns symmetric?\n",
    "\n",
    "One quick way of doing this is to determine how many data points we have on either side of the mean here. We have a bit more then 5,000 data points here. The below code takes the count of data points greater than the mean and divides it by the total data points. This will give us the percentage of data points greater than the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(len(df[df.SP500 > df.SP500.mean()])) / (len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're getting about 52.7% of data points being greater than the mean, which shows we have a slightly positive skew to this dataset. We can't rule out symmetric returns based on this since it is only a sample of data and is reasonably close to the 50% mark. This makes it hard to say for certain whether S&P 500 returns are symmetric or not, but it is still a reasonable assumption to make here. Also, keep in mind there should be a slight bias towards positive returns anyways, if for no other reason than some drift from inflation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Is Volatility constant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vols = pd.DataFrame(df.SP500.rolling(50).std()).rename(columns={\"SP500\": \"S&P 500 STD\"})\n",
    "# set figure size\n",
    "plt.figure(figsize=(12, 5))\n",
    "# plot using rolling average\n",
    "sns.lineplot(\n",
    "    x=\"Date\",\n",
    "    y=\"S&P 500 STD\",\n",
    "    data=vols,\n",
    "    label=\"S&P 500 50 day standard deviation rolling avg\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we calculate a rolling average of the standard deviation and then make a line chart of it. It can be clearly seen that volatility is anything but constant. This adds another layer of complexity to modeling stock returns, especially the many models which assume constant volatility "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Are Stock Returns Normally Distributed?\n",
    "The normal distribution is one of the most common distributions used in modeling random variables. Indeed, many phenomena in the natural and social sciences can be modeled by normal distribution. One of the great advantages of the normal distribution is its simplicity. We can completely describe a normal distribution through two numbers: one for the center of the distribution and one for the uncertainty about that center. The first number refers to the mean, and the second number refers to the standard deviation.\n",
    "\n",
    "Once we have these two numbers, we can draw inferences, estimate percentiles, compute probabilities that a point falls within a region, and more. If our data is well-represented by the normal distribution, then we can confidently use the mean and standard deviation to report our portfolio expected returns and volatilities. If our data is not well represented by the normal distribution, then we need to find other distributions that are more suitable. Thus, when we have a distribution of stock returns, for example, we'll want to start this assessment by visualizing the returns to see if they appear to be normal. Of course, we can follow this up with more quantitative assessments by running statistical tests.\n",
    "\n",
    "We can visualize the data using the hist() method. We pass in bins = 100 as a parameter to determine the amount of buckets to place the data in. The more bins you have, the more granular the data will look in a histogram. Increasing the bins too much may result in slightly noisy data, which will make it tougher to determine a normal distribution. The chart in Figure 3 looks like it could be normally distributed, but we need to be a little more scientific to determine if that's actually the case or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Conducting a normality test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.normaltest((np.array(df.SP500)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `normaltest` method here to determine if the sample data could fit a normal distribution. This method uses D’Agostino and Pearson’s normality test, which combines skew and kurtosis to produce an omnibus test of normality.\n",
    "\n",
    "The null hypothesis of this test is that the sample data fits a normal distribution. Let's assume we want to be 90% confident this data fits a normal distribution. We can compare this to the p-value to see if it's greater than 90%. In this case, the value, 2.43e-252, is extremely small, which leads us to reject the null hypothesis that this data fits a normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Testing Skewness and Kurtosis\n",
    "As one added testing step, we can test the skewness and kurtosis of our distribution using the Jarque-Bera test. The test statistic will always be greater than zero. The further the test statistic is from zero, the more likely the sample data does not match a normal distribution.\n",
    "\n",
    "Lucky for us, Python has another library for us to use here which really simplifies the analysis. From the `scipy.stats` library, we can use the `jarque_bera` method directly to our data to get test statistic \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.jarque_bera((np.array(df.SP500))).pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p value of our data here shows once and for all that there is virtually zero chance our sample data is normally distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Where Does Our Gaussian Distribution Break Down?\n",
    "So according to the normality test, our data is not normally distributed despite the histogram looking like it may be. So, why is the data failing the normality test? The answer likely comes down to fat tails. Fat tails essentially means that extreme events +/-3 standard deviations away from the mean) are more likely than the normal distribution would imply.\n",
    "\n",
    "Assuming a normal distribution with a mean of 0.00028 and standard deviation of 0.012, we can determine the probability of any return given that the returns fit a normal distribution.\n",
    "\n",
    "To determine how many standard deviations away from the mean a specific number is, we need to use \n",
    "\n",
    "$\\tfrac{X - \\bar{X}}{\\textrm{Sample standard deviation}}$\n",
    "\n",
    " Let's do this for the min and max of the sample data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfMax = df.SP500.max()\n",
    "dfMin = df.SP500.min()\n",
    "print(\n",
    "    \"Min return of sample data is %.4f and the maximum return of sample data is %.4f\"\n",
    "    % (dfMin, dfMax)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.SP500.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.SP500.min() - df.SP500.mean()) / df.SP500.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.SP500.max() - df.SP500.mean()) / df.SP500.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over the last 20 years, S&P 500 has had a maximum daily return of 10.96% and a minimum daily return of -12.77%. If we use the formula to determine standard deviations from the mean, we get -10.5 and 8.9 standard deviations away from the mean for the minimum and maximum, respectively. These standard deviations are humongous when compared to the normal distribution. We can see this analytically when we plug in the z score to the `norm.cdf` method to determine the probability this value could be in a normal distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.norm.cdf(-10.45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implies that the chance we could have a move as small as -12.77%, is 7.326261431744285e-26. This probability is so low that we would never expect an event like this to happen in our lifetime. We have multiple events like this, as illustrated by the minimum and maximum.\n",
    "\n",
    "Going further with this idea, based on normal distribution z tables, we would expect 99.7% of our data points to be within +/- 3 standard deviations from the mean. Let's determine this for our sample data. First off, we need to find the cut-off values at +/- 3 standard deviations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(3 * df.SP500.std()) + df.SP500.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(-3 * df.SP500.std()) + df.SP500.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above two calculations would imply that 99.7% of all of our data points should be in between -0.0364 and 0.03699. Since we have 5,031 data points, we would expect about 15 of them to be outside of that range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df[\"SP500\"] > 0.03699) | (df[\"SP500\"] < -0.0364)].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[(df[\"SP500\"] > 0.05) | (df[\"SP500\"] < -0.05)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not only do we get 85 values outside of our 3 standard deviation range, but we also get 36 values outside of +/- 5%, though you would almost never expect one of these events over 20 years, given a normal distribution.\n",
    "\n",
    "In this next video, we go over how to leverage Python in order to test for normality of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import VimeoVideo\n",
    "\n",
    "VimeoVideo(\"706653140\", h=\"47eb01d16b\", width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [Access video transcript here](https://drive.google.com/file/d/1aJ1WcWvEEM5cUnJZm4lDdyOug1j2E9Rs/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All this analysis does is basically prove that we have fat tails in our sample data, which is why the normal distribution is not suitable for modeling stock returns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Non-Gaussian Distributions\n",
    "\n",
    "One potential alternative distribution we could use to forecast stock returns is the Student's t-distribution. This is very similar to a normal distribution except it has heavier tails. Theoretically, this sounds perfect for daily returns based on what we’ve seen up to this point.\n",
    "\n",
    "Since we have 5,031 data points, we have 5,030 degrees of freedom. We can plug these numbers right into `stats.t.rvs` method to see a sample t-distribution and chart it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.t.rvs(df=5030, size=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate t distribution with sample size 10000\n",
    "x = stats.t.rvs(df=5030, size=10000)\n",
    "\n",
    "# create plot of t distribution\n",
    "plt.hist(x, density=True, edgecolor=\"black\", bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the `ttest_ind` method to determine the probability that our returns came from the same sample distribution as in above figure\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_stat, p = stats.ttest_ind(df[\"SP500\"], stats.t.rvs(df=5030, size=5031))\n",
    "print(f\"t={t_stat}, p={p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we get a p-value of 0.866, which means we would reject the null hypothesis at a 95% confidence level that these values come from the same data as the sample. There are two main things to point out here:\n",
    "\n",
    "* While the p-value is less than 0.95, it’s still not too far off, and it's magnitudes closer to being true than the normal distribution we showed before. This indicates that this distribution is much better to use when modeling daily returns than the normal distribution. \n",
    "* This p-value is highly reliant on the sample data we have here. Every time we run that cell, we get different sample data, which also means different p-values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Conclusion\n",
    "\n",
    "In this lesson, we focused for the first time on comparing daily stock return data to normal distributions. We also touched on other potential distributions we could use to model this data. In the next module, we will take what we've learned thus far and apply it to a portfolio setting instead of just looking at individual assets in isolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References**\n",
    "\n",
    "- D'Agostino, Ralph B. “An Omnibus Test of Normality for Moderate and Large Size Samples.” *Biometrika*, vol. 58, no. 2, 1971, pp. 341–348, https://doi.org/10.1093/biomet/58.2.341."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
