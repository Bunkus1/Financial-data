{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Othbq-05bzk"
   },
   "source": [
    "## FINANCIAL DATA\n",
    "MODULE 6 | LESSON 2\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3207b418-f193-4642-9313-665839b10dd4",
    "_uuid": "a2732e4816cf3356a500c1623263d2337c8827c7",
    "id": "mNHetiXpNKN6"
   },
   "source": [
    "<h1> <b> CORRELATIONS AND TIME SERIES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mtAwSyrZ5e8e"
   },
   "source": [
    "\n",
    "|  |  |\n",
    "|:---|:---|\n",
    "|**Reading Time** |  30 minutes |\n",
    "|**Prior Knowledge** | Basic Python  |\n",
    "|**Keywords** |Simple Moving Average (SMA), Exponential Moving Average (SMA), Pearson correlation, Spearman correlation, <br> Kendall correlation, Single exponential smoothing, Double (Holt's) exponential smoothing, Triple (Holt-Winters) exponential smoothing |   \t\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SogkwrbiGNWu"
   },
   "source": [
    "\n",
    "*In the last lesson, we downloaded batches of data using an API and analyzed it for the frequency of various sentiments. In this lesson, we learn about time series analysis and especially moving averages, which includes simple moving averages and exponential moving averages of various kinds. Then, we review the three most used types of correlation (Pearson, Kendall, and Spearman) in preparation for analyzing the correlations among various cryptocurrencies.*\n",
    "\n",
    "###### These lesson notes are a fork of [this notebook](https://www.kaggle.com/code/dbarkhorn/crypto-correlation/notebook) by Danbarkhorn, which is released under the Apache 2.0 open source license."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kzUmit9IQH3L"
   },
   "source": [
    "## 1. Time Series Analysis and Smoothing \n",
    "\n",
    "Financial engineering often relies on historical data, so a solid understanding of time series analysis is crucial. Most of what you need to know for this lesson about forecast quality metrics and smoothing techniques will be found [in the required reading (section 1: \"Introduction\" and section 2: \"Move, smoothe, evaluate\").](https://www.kaggle.com/kashnitsky/topic-9-part-1-time-series-analysis-in-python)\n",
    "\n",
    "\n",
    "Get further information on Double and Triple Exponential Smoothing from [this required reading.](https://online.stat.psu.edu/stat501/node/1001)\n",
    "\n",
    "As you will see, moving averages (both exponential and simple moving averages) will come into play in many ways in the later lesson about technical analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_g6fiFyCI2ya"
   },
   "source": [
    "## 2. Correlations\n",
    "\n",
    "There are different approaches to assessing the strength of the relationship between two variables, and each has its own strengths and weaknesses. [See the following required reading before we calculate the different correlation metrics between cryptocurrencies in the rest of this notebook.](https://www.kaggle.com/kiyoung1027/correlation-pearson-spearman-and-kendall/report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "14791e9e-f541-4188-88ee-84713814287e",
    "_uuid": "2cb534a31814f0e8974f4f969455027879afdb20",
    "id": "RIQrNaoSNKN8"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import correlate\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from statsmodels.tsa.stattools import adfuller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a1b04096-e52d-4b6b-95bd-d55f7d7cc903",
    "_uuid": "9577e05b865aac845fbd39c4a1358dd9e6c97525",
    "id": "aMYWICVUNKN9"
   },
   "source": [
    "# 3. Cryptocurrency Correlations\n",
    "\n",
    "The data for this lesson comes from Kaggle and has been downloaded to your virtual machine. You can consult the original source [here](https://www.kaggle.com/dbarkhorn/crypto-correlation/data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4f179bc5-2318-4cf2-a2ec-b600c0237cb0",
    "_uuid": "aef36b7e96463d18de1138c46e37569b8638e3b0",
    "id": "zk6rUHqyNKN-"
   },
   "outputs": [],
   "source": [
    "crypto = {}\n",
    "\n",
    "crypto[\"bitcoin\"] = pd.read_csv(\"./data/bitcoin_price.csv\")\n",
    "crypto[\"bitcoin_cash\"] = pd.read_csv(\"./data/bitcoin_cash_price.csv\")\n",
    "crypto[\"dash\"] = pd.read_csv(\"./data/dash_price.csv\")\n",
    "crypto[\"ethereum\"] = pd.read_csv(\"./data/ethereum_price.csv\")\n",
    "crypto[\"iota\"] = pd.read_csv(\"./data/iota_price.csv\")\n",
    "crypto[\"litecoin\"] = pd.read_csv(\"./data/litecoin_price.csv\")\n",
    "crypto[\"monero\"] = pd.read_csv(\"./data/monero_price.csv\")\n",
    "crypto[\"nem\"] = pd.read_csv(\"./data/nem_price.csv\")\n",
    "crypto[\"neo\"] = pd.read_csv(\"./data/neo_price.csv\")\n",
    "crypto[\"numeraire\"] = pd.read_csv(\"./data/numeraire_price.csv\")\n",
    "crypto[\"ripple\"] = pd.read_csv(\"./data/ripple_price.csv\")\n",
    "crypto[\"stratis\"] = pd.read_csv(\"./data/stratis_price.csv\")\n",
    "crypto[\"waves\"] = pd.read_csv(\"./data/waves_price.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5b76c6bf-a268-4b3a-810e-c3962c0b83c2",
    "_uuid": "28f3609438af096646bae77b5ce9250f5e885c76",
    "id": "SZRQQfEhNKN-"
   },
   "outputs": [],
   "source": [
    "# For this analysis, (*) we will only be looking at closing price to make things more manageable\n",
    "for coin in crypto:\n",
    "    for column in crypto[coin].columns:\n",
    "        if column not in [\"Date\", \"Close\"]:\n",
    "            crypto[coin] = crypto[coin].drop(column, 1)\n",
    "    # Make date the datetime type and reindex\n",
    "    crypto[coin][\"Date\"] = pd.to_datetime(crypto[coin][\"Date\"])\n",
    "    crypto[coin] = crypto[coin].sort_values(\"Date\")\n",
    "    crypto[coin] = crypto[coin].set_index(crypto[coin][\"Date\"])\n",
    "    crypto[coin] = crypto[coin].drop(\"Date\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4dee53f0-a55d-4199-a7b2-e71e9c7cff10",
    "_uuid": "907c912c55066968c8834a7d28ee2f4d3491650d",
    "id": "ZTxsASeiNKN_"
   },
   "outputs": [],
   "source": [
    "for coin in crypto:\n",
    "    print(coin, len(crypto[coin]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0f17a898-a73a-400a-b7c4-56652ffafac2",
    "_uuid": "c8f975b3b4899e6b0fc1f36c1e2950ba5d57ebf0",
    "id": "0YD3FlhbNKN_"
   },
   "source": [
    "**Note:** \n",
    "As this is a fixed set of data, the coins numeraire, iota, and bitcoin_cash all were all relatively young and therefore do not have many data points. (*) So let's omit these currencies and for the time being consider only the most recent 350 data points for the remaining currencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "456ee863-40c5-485f-abba-4abe20fd6547",
    "_uuid": "8195ed09a53cb263465f4974c8c8316616d3c971",
    "id": "KLHxP06kNKOA",
    "tags": []
   },
   "outputs": [],
   "source": [
    "del crypto[\"bitcoin_cash\"], crypto[\"numeraire\"], crypto[\"iota\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "54f10987-4b5f-47d5-8303-6c9505904897",
    "_uuid": "ce89402e44a745e20b7db68aee253f6967459fb4",
    "id": "r69IDa8gNKOB",
    "tags": []
   },
   "outputs": [],
   "source": [
    "cryptoAll = {}  # for later on\n",
    "\n",
    "for coin in crypto:\n",
    "    cryptoAll[coin] = crypto[coin]\n",
    "    crypto[coin] = crypto[coin][-350:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "86499dae-151d-44f3-b91f-c16a1f3bf770",
    "_uuid": "c1cc92ce893188aa00f780bb335fff8ec90c1371",
    "id": "pAfBNh8tNKOB"
   },
   "source": [
    " As previously stated, the goal of this analysis is to create a correlation matrix for these currencies. One way to find correlation between time series is to look at *cross-correlation* of the time series. Cross-correlation is computed between two time series using a lag, so when creating the correlation matrix, (*) we need to specify the correlation as well as the lag.\n",
    " \n",
    " Before computing the cross-correlation, it is important to have \"wide-sense stationary\" (often just called stationary) data. There are a few ways to make data stationary--one of which is through differencing. But even after this, it is famously difficult to avoid spurious correlations between time series data that are often caused by autocorrelation. See this article for an in-depth analysis of how spurious correlations arise and how to avoid them: [\"Dangers and Uses of Cross-Correlation in Analyzing Time Series in Perception, Performance, Movement, and Neuroscience: The Importance of Constructing Transfer Function Autoregressive Models.\"](https://doi.org/10.3758/s13428-015-0611-2) (This is a free, shareable article, but it is not a required reading.)\n",
    " \n",
    "For now, (*) we will employ daily differencing (as it is not seasonal) and test for stationarity to prepare for cross-correlation testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "01a7f876-2337-493c-b750-e284582286a8",
    "_uuid": "5ec21995a731253debc3c0de5ebffa37400b75bb",
    "id": "-HokzqAlNKOC"
   },
   "outputs": [],
   "source": [
    "# Differencing\n",
    "for coin in crypto:\n",
    "    crypto[coin][\"CloseDiff\"] = crypto[coin][\"Close\"].diff().fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c1be4bf9-d292-4564-8b5f-dcc62f4ac859",
    "_uuid": "77872d8d327df677f569c1f0623e22b2329b1845",
    "id": "oO8PCiobNKOC"
   },
   "source": [
    "Now, let's take a preliminary look at how our graph appears. Further steps may have to be taken to make the data stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "04095141-8b27-4b83-a2cf-4712a429eb69",
    "_uuid": "45b7ce3844a37deba2f3728d97093843a4a37d70",
    "id": "dfJBZKPUNKOC"
   },
   "outputs": [],
   "source": [
    "for coin in crypto:\n",
    "    plt.plot(crypto[coin][\"CloseDiff\"], label=coin)\n",
    "plt.legend(loc=2)\n",
    "plt.title(\"Daily Differenced Closing Prices\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "82ed118a-1576-4882-acf8-4d02ae71f26f",
    "_uuid": "92aa9a6e1a62b242971216ef5efe3e6c2cc3c711",
    "id": "gxqjh0QYNKOC"
   },
   "source": [
    "Note:\n",
    "Here we see that one of the coins (Bitcoin) has much larger spikes than the other coins. While this may still have given us stationarity, it may be useful to also look at the percentage change per day of the time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9bc6c911-438a-4eb2-b952-f43322d7b950",
    "_uuid": "00b136164062296edc7c3b8197b2053815d9e8b8",
    "id": "g5G-w2n7NKOC"
   },
   "outputs": [],
   "source": [
    "# Percent Change\n",
    "for coin in crypto:\n",
    "    crypto[coin][\"ClosePctChg\"] = crypto[coin][\"Close\"].pct_change().fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "95da6f84-2619-406b-8027-84e784b14c23",
    "_uuid": "b8e75f104b7e4805a307e74eb0bfba10463c33b8",
    "id": "-E_4hmlRNKOD"
   },
   "outputs": [],
   "source": [
    "for coin in crypto:\n",
    "    plt.plot(crypto[coin][\"ClosePctChg\"], label=coin)\n",
    "plt.legend(loc=2)\n",
    "plt.title(\"Daily Percent Change of Closing Price\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d750adbe-bad9-4498-aed0-660007861d06",
    "_uuid": "9b483c39d4ebba5b227e6fb9e2725fe693a517dd",
    "id": "XMzEf3CwNKOD"
   },
   "source": [
    "**Note:**\n",
    "As before, we still have some very large peaks, but overall, the data looks more contained than it was previously. Most importantly, we do not have a single coin dominating the others.\n",
    "\n",
    "Focus on one particular part of the graph to get an idea of any correlation going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8c666d05-28e0-4395-b61a-0787594f1c36",
    "_uuid": "5af1356c4181985b6496fe624015605bfc902cb9",
    "id": "X2jtNjLFNKOJ"
   },
   "outputs": [],
   "source": [
    "for coin in crypto:\n",
    "    plt.plot(crypto[coin][\"ClosePctChg\"][-30:], label=coin)\n",
    "plt.legend(loc=2)\n",
    "plt.title(\"Daily Percent Change of Closing Price\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "17226f16-1630-4b14-ac56-d50eae9fefa2",
    "_uuid": "71c78ffa69bffb0d5116c4a0f99df0e9091519b8",
    "id": "KCp1iJCfNKOK"
   },
   "source": [
    "**Note:**\n",
    "Here it seems as if we do in fact have some correlation going on, which is what we were hoping for.\n",
    "\n",
    "It is also important to note that a number of other types of differencing or normalizations could have been applied. As this is only a preliminary analysis, this may not end up being the best way to prepare the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "74e24ae7-08d5-4f09-8941-b234b9d10e40",
    "_uuid": "a8fb0bf39183d08a450e3ed0ca4366aca071caec",
    "id": "KNmEZSOINKOK"
   },
   "source": [
    "## 4. Stationarity\n",
    "\n",
    "You will learn all about stationarity in the Econometrics course. In short, the mean and variance of a stationary series does not change over time. You will not be quizzed on stationarity per se in this lesson, but read through this section so that you will have been exposed to the concept by the time you get to Econometrics--and because we should perform these tests on our Bitcoin data anyway.\n",
    "<br>\n",
    "\n",
    "We can test for stationarity by using *unit root tests*. One of which is the Augmented Dickey-Fuller Test. Dickey Fuller utilizes the following regression.\n",
    "\n",
    "$$ Y'_t \\space = \\space \\phi Y_{t-1} \\space + \\space b_1 Y'_{t-1} \\space + \\space b_2 Y'_{t-2} \\space +...+ \\space b_p Y'_{t-p} $$\n",
    "$$ $$\n",
    "$$ Y'_t \\space = \\space Y_t \\space - \\space Y_{t-1} $$\n",
    "\n",
    "Using the Augmented Dickey Fuller test, we look at the following statistic.\n",
    "\n",
    "$$ DF_t \\space = \\space \\frac{\\hat{\\phi}}{SE(\\hat{\\phi})} $$\n",
    "\n",
    "Then, this statistic is compared to a table given by Dickey Fuller. Given the number of samples, we can guess with a % certainty whether or not our data is stationary.\n",
    "\n",
    "$$ H_{0} \\space : data \\space is \\space nonstationary $$\n",
    "$$ H_{A} \\space : data \\space is \\space stationary $$\n",
    "\n",
    "To check these hypotheses, we look at the p-value of our given statistic (*) [using a table](https://www.real-statistics.com/statistics-tables/augmented-dickey-fuller-table/). In the table, we look at model 1 (the middle table) with 250 < n < 500. From here, we can see that in order to know with 1% certainty whether or not our data is stationary, we can compare our ð·ð¹ð‘¡ statistic to the values -3.457 and -3.443. Since our calculated values are less than these critical values from the table, we have a significant result, i.e., our time series are stable.\n",
    "\n",
    "[^@#$%]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d5645535-0666-493c-81a8-40e58b4fcf7e",
    "_uuid": "5cce64817a9a862712e888fa109271f087f0c8ae",
    "id": "TTYz1bY1NKOK"
   },
   "outputs": [],
   "source": [
    "for coin in crypto:\n",
    "    print(\"\\n\", coin)\n",
    "    adf = adfuller(crypto[coin][\"ClosePctChg\"][1:])\n",
    "    print(coin, \"ADF Statistic: %f\" % adf[0])\n",
    "    print(coin, \"p-value: %f\" % adf[1])\n",
    "    print(coin, \"Critical Values\", adf[4][\"1%\"])\n",
    "    print(adf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "037df3d9-63c2-4b34-8bb7-28f1fab55811",
    "_uuid": "632dcbcf91628057de006327eddee3aeec12a3f9",
    "id": "dzDx-5LINKOK"
   },
   "outputs": [],
   "source": [
    "for coin in crypto:\n",
    "    print(\"\\n\", coin)\n",
    "    adf = adfuller(crypto[coin][\"CloseDiff\"][1:])\n",
    "    print(coin, \"ADF Statistic: %f\" % adf[0])\n",
    "    print(coin, \"p-value: %f\" % adf[1])\n",
    "    print(coin, \"Critical Values\", adf[4][\"1%\"])\n",
    "    print(adf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c22d5c8c-0ab3-45ba-a860-e5d663227dd3",
    "_uuid": "0bfa930be30867c47efe8d14dfd76c39ab384df0",
    "id": "_aFrbENoNKOK"
   },
   "source": [
    "**Note:**\n",
    "Here we see that our data is very stationary. This is clear because of the extremely low p-values. \n",
    "\n",
    "It is important here to note there are other ways to detrend other than looking at differenced data or percent change. However, some of these methods would not have proven fruitful for this dataset. Take, for example, using the residuals of this data based on a simple linear regression. This can be easily done using scikit learn's linear regression tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "98298af9-c255-4944-abb6-cac6ace164f8",
    "_uuid": "a5905bf439fc3f62de223d9fdcfc6c7d58b5a5c1",
    "id": "3f7a5pElNKOK"
   },
   "outputs": [],
   "source": [
    "for coin in crypto:\n",
    "    model = LinearRegression()\n",
    "    model.fit(np.arange(350).reshape(-1, 1), crypto[coin][\"Close\"].values)\n",
    "    trend = model.predict(np.arange(350).reshape(-1, 1))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(trend, label=\"trend\")\n",
    "    plt.plot(crypto[coin][\"Close\"].values)\n",
    "    plt.title(coin)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(crypto[coin][\"Close\"].values - trend, label=\"residuals\")\n",
    "    plt.title(coin)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ad9699d3-31d4-43db-86e2-35462642df35",
    "_uuid": "a7f3b3ec5c2903ce35be882956f4e82af06b5883",
    "id": "VrM6gpSzNKOK"
   },
   "source": [
    "**Note:**\n",
    "We are getting ineffective results: Since many of these currencies only started gaining traction recently, this shows that the preferred method was what was done originally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "df4f4823-f2b0-400c-a711-0c5ee0a99ca0",
    "_uuid": "44d995d9c2ccee43999a50d58ecfc1855de97a90",
    "id": "E5ls8lJXNKOK"
   },
   "source": [
    "And finally, the actual correlations analysis. We use scipy's correlate function. The cross-correlation will tell us if we should lag one of the series. Cross-correlation is often used in signal processing to match signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "898a12be-2143-4cfe-901a-89653c719a3d",
    "_uuid": "a48fcbbdeda7d86ffd0d5bb38d82588506e49e9d",
    "id": "DYZetLuNNKOK"
   },
   "outputs": [],
   "source": [
    "corrBitcoin = {}\n",
    "corrDF = pd.DataFrame()\n",
    "\n",
    "for coin in crypto:\n",
    "    corrBitcoin[coin] = correlate(\n",
    "        crypto[coin][\"ClosePctChg\"], crypto[\"bitcoin\"][\"ClosePctChg\"]\n",
    "    )\n",
    "    lag = np.argmax(corrBitcoin[coin])\n",
    "    laggedCoin = np.roll(crypto[coin][\"ClosePctChg\"], shift=int(np.ceil(lag)))\n",
    "    corrDF[coin] = laggedCoin\n",
    "\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.plot(laggedCoin)\n",
    "    plt.plot(crypto[\"bitcoin\"][\"ClosePctChg\"].values)\n",
    "    title = coin + \"/bitcoin PctChg lag: \" + str(lag - 349)\n",
    "    plt.title(title)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6fo4cznZNKOL"
   },
   "source": [
    "Now we can look at the correlations among these currencies. \n",
    "We will compute the correlations using three different methods: pearson, spearman, and kendall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "301c87a4-8f33-403e-a282-31454c4d1d9a",
    "_uuid": "0e26d454d17babca828b13291647ecb9fb09911a",
    "id": "slZqeC64NKOL"
   },
   "outputs": [],
   "source": [
    "font = {\n",
    "    \"family\": \"serif\",\n",
    "    \"color\": \"black\",\n",
    "    \"weight\": \"normal\",\n",
    "    \"size\": 20,\n",
    "}\n",
    "\n",
    "plt.matshow(corrDF.corr(method=\"pearson\"))\n",
    "plt.xticks(range(10), corrDF.columns.values, rotation=\"vertical\")\n",
    "plt.yticks(range(10), corrDF.columns.values)\n",
    "plt.xlabel(\"Pearson Correlation\", fontdict=font)\n",
    "plt.show()\n",
    "corrDF.corr(method=\"pearson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b0717b7f-0c5a-4dff-98f6-bb38d3797408",
    "_uuid": "e604b4c698618327912aebcd301f11ca010aedcf",
    "id": "fhR739m3NKOL"
   },
   "outputs": [],
   "source": [
    "plt.matshow(corrDF.corr(method=\"spearman\"))\n",
    "plt.xticks(range(10), corrDF.columns.values, rotation=\"vertical\")\n",
    "plt.yticks(range(10), corrDF.columns.values)\n",
    "plt.xlabel(\"Spearman Correlation\", fontdict=font)\n",
    "plt.show()\n",
    "corrDF.corr(method=\"spearman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a7f41fd6-5b60-4969-9da1-f5588c692878",
    "_uuid": "ef1a8ccaba72f43c064d7a3a5cd817fa5b683bb7",
    "id": "lyV0Qk8GNKOL"
   },
   "outputs": [],
   "source": [
    "plt.matshow(corrDF.corr(method=\"kendall\"))\n",
    "plt.xticks(range(10), corrDF.columns.values, rotation=\"vertical\")\n",
    "plt.yticks(range(10), corrDF.columns.values)\n",
    "plt.xlabel(\"Kendall Correlation\", fontdict=font)\n",
    "plt.show()\n",
    "corrDF.corr(method=\"kendall\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6cd26564-7423-4ba5-a56a-749b1d03d844",
    "_uuid": "8ed3bb49289a73c2a2d96d3e2605c1f08ccd0c44",
    "id": "3IpHC0ClNKOL"
   },
   "source": [
    "**Note:**\n",
    "We see here that with all of these correlation methods we get about the same results, but with slightly different magnitudes.\n",
    "Also, we should note that there are *no* correlations greater than .5\n",
    "This is contrary to what may be found if we were to, for example, take the correlation of the nonstationary datasets. This suggests (*) we have avoided spurious correlations between currencies. Additionally, note that only two of the currencies showed stronger correlations with lagged data. This makes sense as these currencies have shown to be very responsive to media in the recent past.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e3ecddef-ac04-4846-b852-30ec8f097cfc",
    "_uuid": "4c46ee6c32798537d5152c1077cf82daaa912529",
    "id": "121zVXfSNKOL"
   },
   "source": [
    "## 5. Conclusion\n",
    "\n",
    "In this lesson, we reviewed moving averages and correlations in order to analyze the correlations among Bitcoin time series data. In the next lesson, we continue to use moving averages as an input to various technical analysis approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ikWJtrs7Q6G"
   },
   "source": [
    "**References**\n",
    "\n",
    "* Danbarkhorn. \"Crypto-correlation.\" *Kaggle*. 10 Sep 2017. https://www.kaggle.com/code/dbarkhorn/crypto-correlation/notebook.\n",
    "\n",
    "* Dean, Roger, and William Dunsmuir. \"Dangers and Uses of Cross-Correlation in Analyzing Time Series in Perception, Performance, Movement, and Neuroscience: The Importance of Constructing Transfer Function Autoregressive Models.\" *Behavioral Research Methods*, vol. 48, 2016, pp. 783â€“802. https://doi.org/10.3758/s13428-015-0611-2.\n",
    "\n",
    "* Kashnitsky, Yury. \"Topic 9. Part 1. Time series analysis in Python.\" *Kaggle*. 3 Jan 2021. https://www.kaggle.com/kashnitsky/topic-9-part-1-time-series-analysis-in-python.\n",
    "\n",
    "* Rooney. \"Correlation (Pearson, Spearman, and Kendall).\" *Kaggle*. 31 Dec 2019. https://www.kaggle.com/kiyoung1027/correlation-pearson-spearman-and-kendall/report.\n",
    "\n",
    "* Simon, Laura, and Derek Young. \"14.5.2 - Exponential Smoothing.\" *Penn State Statistics Online*. https://online.stat.psu.edu/stat501/node/1001."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Footnotes**\n",
    " \n",
    " - In compliance with the Apache License 2.0, the (*) marks the places where changes were made to the original notebook.\n",
    " - You can find the [APACHE LICENSE, VERSION 2.0 here.](https://www.apache.org/licenses/LICENSE-2.0) \n",
    " - **NOTE:** The above Apache license notice is copied here to comply with its requirements, but it does **not** apply to the content in these lesson notes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Copyright Â© 2022 WorldQuant University. This\n",
    "content is licensed solely for personal use. Redistribution or\n",
    "publication of this material is strictly prohibited.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Financial Data_Module 6_Lesson 2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
