{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jr7rY8RMxQYo"
   },
   "source": [
    "\n",
    "## FINANCIAL DATA\n",
    "MODULE 6 | LESSON 1\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oGHv4-wGGmgg"
   },
   "source": [
    "# **SENTIMENT ANALYSIS: JSON AND TWEETS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RHLrt9Mq1ehp"
   },
   "source": [
    "\n",
    "|  |  |\n",
    "|:---|:---|\n",
    "|**Reading Time** |  25 minutes |\n",
    "|**Prior Knowledge** | Basic Python, Sentiment Analysis (API)  |\n",
    "|**Keywords** |Affect, JSON (JavaScript Object Notation), Application Programming Interface (API), NLTK (Natural Language Toolkit) |   \t\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dn6nS5dJyiJO"
   },
   "source": [
    "\n",
    "*In this module, we are going to register with Twitter as applications developers in order to get the keys and access tokens to use the Twitter application programming interface (API). Then, we can search tweets from the past seven days that mention whatever query word we like, for example a company's name or ticker. Once we have this dataset of tweets, we can perform our sentiment analysis: How frequently are various basic emotions evidenced in the text of the tweets that mention the company we are interested in? But tweets and their relevant data (country of origin, creation time, language) are stored in JSON objects. JSON objects are a very common format for data, so we will invest some time in understanding the structure and syntax of this data object.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PnBECf5_yGER"
   },
   "source": [
    "## 1. The Twitter API\n",
    "API stands for application programming interface. An API is how one computer or program \"interfaces\" or interacts with another computer or program. That means it is also how we users interface with computers or programs. The Twitter API is how we users (use the Python noteboooks on our computers to) interface with the Twitter database of tweets.\n",
    "\n",
    "* Go to Twitter.com and get a Twitter account if you don't have one yet.\n",
    "* Sign in to the developer site [https://developer.twitter.com/en](https://developer.twitter.com/en) using your Twitter account\n",
    "* Now you need to register as an applications (app) developer\n",
    "* Go to [https://apps.twitter.com/app/new](https://apps.twitter.com/app/new) \n",
    "* Give your new app a name and a description - you can be creative\n",
    "* Enter anything for website \n",
    "* Leave callback url blank\n",
    "* Read and accept the terms and conditions to create an account\n",
    "* Then, click on the \"Keys and Access Tokens\" tab\n",
    "* Copy the two consumer keys to a text file\n",
    "* Scroll down and click \"Generate Access key and Secret\"\n",
    "* Copy the two access keys to the text file\n",
    "\n",
    "NOTE: With this free Twitter API v2, you will get an error unless your search dates are within the last seven days. Also, you can only download 500,000 tweets per month. For this reason, be conservative and restrict your start and end times, especially if you are investigating tweets for very well-known companies, like AAPL (Apple) and GOOG (Alphabet / Google). Just a 30-minute block of time (the difference between start time and end time is 30 minutes) is actually sufficient to do some exploratory analysis on such large companies. Much smaller companies may require several hours or even days. \n",
    "\n",
    "Note too that using the client.sample() method does not count against the 500K tweet limit. These tweets may not include a reference to the company you are investigating, but it could still be interesting. Once downloaded, you can also filter these tweets for whatever interests you.\n",
    "\n",
    "Twarc is a library that will make it a lot easier for us to search and access Twitter through its API. [For more background on Twarc, see this site,](https://twarc-project.readthedocs.io/en/latest/twarc2_en_us/) [and this site.](https://github.com/DocNow/twarc/blob/4bff06bb8f2e279ea0a2064d6bfaec3c9b4b3aff/docs/api/library.md)\n",
    "\n",
    "Note from the documentation above that Twarc is a command line tool, but we have adapted it with [this code](https://github.com/twitterdev/getting-started-with-the-twitter-api-v2-for-academic-research/blob/f3b5b40d197ecaac133299eea46fefcced569d21/modules/6b-labs-code-standard-python.md) to run from a notebook as part of a function we define. \n",
    "\n",
    "You will notice the reference to JSON objects. Luckily, we use Twarc to convert these into a `DataFrame`. Still, in the next section, you will need to become more familiar with JSON objects given how prevalent they are as a data type.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import VimeoVideo\n",
    "\n",
    "VimeoVideo(\"714293481\", h=\"e42c792a9e\", width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [Access video transcript here](https://drive.google.com/file/d/1i7aDRSZgaHg9w0i_wjOutoi87gjbovwA/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ugYUJDvd4cFf"
   },
   "source": [
    "## 2. Quantifying Affect \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_W33lLHJcosZ"
   },
   "source": [
    "\n",
    "The NRCLex library measures the emotion or \"affect\" in a piece of text, so we will use it for the actual sentiment analysis in this program.\n",
    "[Find more background on the NRCLex library you will be using here.](https://pypi.org/project/NRCLex/)\n",
    "\n",
    "And [here is information about another similar approach to sentiment analysis.](https://www.digitalocean.com/community/tutorials/how-to-perform-sentiment-analysis-in-python-3-using-the-natural-language-toolkit-nltk)\n",
    "After getting familiar with the code and information in this notebook, please use the code provided in the article in your own notebook to try it for yourself, and discuss any issues or insights you have with your fellow students on the forum. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "REvhQM0x2F0O"
   },
   "source": [
    "## 3. JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gAq2RaR42Gto"
   },
   "source": [
    "JSON is an important format for structuring data. <br>\n",
    "Read these required readings for an introduction to JSON: <br>\n",
    "https://www.geeksforgeeks.org/javascript-json/ <br>\n",
    "https://www.geeksforgeeks.org/json-data-types/ <br>\n",
    "<br>\n",
    "\n",
    "For the information you need about using JSON in Python, see this page of required reading:<br>\n",
    "https://www.geeksforgeeks.org/python-json/?ref=lbp <br>\n",
    "Please also read each of the following 5 pages, which are a continuation of the one above (also required reading).<br>\n",
    "https://www.geeksforgeeks.org/working-with-json-data-in-python/?ref=lbp <br>\n",
    "https://www.geeksforgeeks.org/read-write-and-parse-json-using-python/?ref=lbp <br>\n",
    "https://www.geeksforgeeks.org/append-to-json-file-using-python/?ref=lbp <br>\n",
    "https://www.geeksforgeeks.org/serializing-json-data-in-python/?ref=lbp <br>\n",
    "https://www.geeksforgeeks.org/deserialize-json-to-object-in-python/?ref=lbp <br>\n",
    "<br>\n",
    "\n",
    "See also these (required reading) pages for clarification of the JSON related methods. <br>\n",
    "https://www.geeksforgeeks.org/json-load-in-python/ <br>\n",
    "https://www.geeksforgeeks.org/json-loads-in-python/ <br>\n",
    "https://www.geeksforgeeks.org/json-dumps-in-python/ <br>\n",
    "https://www.geeksforgeeks.org/json-dump-in-python/ <br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KNtNqpIWNV0f",
    "outputId": "b1b0be29-38fa-4496-9a39-7db7daa92967"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from nrclex import NRCLex\n",
    "from twarc import Twarc2\n",
    "from twarc_csv import DataFrameConverter\n",
    "\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d48FX10isTes"
   },
   "outputs": [],
   "source": [
    "Bearer_Token = \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\"  # Replace the Xs with your new bearer token from the Twitter development site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Mf8W9dS-IN-"
   },
   "outputs": [],
   "source": [
    "def TweetSearch(query, start_time, end_time):\n",
    "    client = Twarc2(bearer_token=Bearer_Token)\n",
    "    converter = DataFrameConverter()\n",
    "\n",
    "    # The search_recent method call the recent search endpoint to get Tweets based on the query, start and end times\n",
    "    search_results = client.search_recent(\n",
    "        query=query, start_time=start_time, end_time=end_time, max_results=50\n",
    "    )\n",
    "\n",
    "    # Twarc returns all Tweets for the criteria set above, so we page through the results\n",
    "    # for count, page in enumerate(search_results):\n",
    "\n",
    "    Tweets = converter.process(search_results)\n",
    "    Tweets_str = Tweets.to_string()\n",
    "\n",
    "    text_object = NRCLex(Tweets_str)\n",
    "    data = text_object.affect_frequencies\n",
    "\n",
    "    emotion_df = pd.DataFrame.from_dict(data, orient=\"index\")\n",
    "    emotion_df = emotion_df.reset_index()\n",
    "    emotion_df = emotion_df.rename(\n",
    "        columns={\"index\": \"Emotion Classification\", 0: \"Emotion Count\"}\n",
    "    )\n",
    "    emotion_df = emotion_df.sort_values(by=[\"Emotion Count\"], ascending=False)\n",
    "\n",
    "    emotion_df.drop(\n",
    "        emotion_df[emotion_df[\"Emotion Classification\"] == \"anticip\"].index,\n",
    "        inplace=True,\n",
    "    )  # This line just fixes a small cosmetic bug in the Twarc library\n",
    "\n",
    "    fig = px.bar(\n",
    "        emotion_df,\n",
    "        x=\"Emotion Count\",\n",
    "        y=\"Emotion Classification\",\n",
    "        color=\"Emotion Classification\",\n",
    "        orientation=\"h\",\n",
    "        width=800,\n",
    "        height=400,\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "id": "BWZIW_6_CXDJ",
    "outputId": "91815fbf-01cb-4b8b-9082-fa0936154dfb"
   },
   "outputs": [],
   "source": [
    "# Specify your query, like the stock ticker or name of the company you are investigating\n",
    "query = \"AAPL lang:en\"\n",
    "\n",
    "# Specify the start and end times in UTC for the time period you want Tweets from\n",
    "# These dates and times must be within the past week!\n",
    "start_time = datetime(2022, 4, 20, 0, 0, 0, 0, timezone.utc)\n",
    "end_time = datetime(2022, 4, 20, 1, 0, 0, 0, timezone.utc)\n",
    "\n",
    "scrape_new_data = False\n",
    "\n",
    "if scrape_new_data:\n",
    "    df_aapl = TweetSearch(query, start_time, end_time)\n",
    "else:\n",
    "    df_aapl = pd.read_csv(\"AAPL_lang_en_emotion.csv\")\n",
    "\n",
    "df_aapl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C5UBYtScUKCI"
   },
   "source": [
    "Now let's choose a different company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "id": "33pPQcG_QmJ1",
    "outputId": "de50ee94-aa45-45b6-9098-5372923a8df9"
   },
   "outputs": [],
   "source": [
    "# Specify your query, like the stock ticker or name of the company you are investigating\n",
    "query = \"IBM lang:en\"\n",
    "\n",
    "# Be default we are using the same start and end time.\n",
    "# This is also intentional because we want a good comparison\n",
    "\n",
    "if scrape_new_data:\n",
    "    df_ibm = TweetSearch(query, start_time, end_time)\n",
    "else:\n",
    "    df_ibm = pd.read_csv(\"IBM_lang_en_emotion.csv\")\n",
    "\n",
    "df_ibm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1gZF3dG_USrT"
   },
   "source": [
    "A very simplistic approach to sentiment analysis might be to buy the stock with the more \"positive\" emotion counts.\n",
    "\n",
    "Alternatively, subtract the negative from the positive for a net positive vs. negative emotion, and buy the stock with the higher of these values--and sell the other one (assuming you are evaluating pairs of stocks).\n",
    " \n",
    "Or, you might do some more analysis, looking at historical levels of these values--and buy when there is a significant increase in a particular value.\n",
    "\n",
    "Which value? And what constitutes significant? You will need to perform a lot of experimentation--testing and backtesting to find your answer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C-zFkHRRuIUZ"
   },
   "source": [
    "## 4. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5OQWilMzuQou"
   },
   "source": [
    "In this lesson, we learned how to download actual tweets and related data from Twitter.com. In order to do this, we needed developer credentials from Twitter to use in the Twitter API. Because tweets (like much of the data we will encounter as data scientists and financial engineers) are stored as JSON objects, we needed to thoroughly understand the JSON syntax and related methods.\n",
    "\n",
    "Then, we focused our tweet search on companies that we were interested in by supplying the company name as a search term. Given the potential volume of tweets, we did not consider reading them; instead, we used a sentiment analysis library to show us the frequency of basic emotions in order to gauge how Twitter users (which potentially include investors, customers, etc.) feel about the company. We assumed that these feelings might help us indicate how, say, the stock will perform in the near future. We also read about a more thorough and technical approach, which includes tokenizing, normalizing, and denoising the text data before using a Naive Bayes Classifier model to determine which words (and/or emoticons) are most informative about whether a tweet is emotionally positive or negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4XAKfIxyzdOJ"
   },
   "source": [
    "<b> References </b> \n",
    "\n",
    "* \"Append to JSON file using Python.\" *Geeks for Geeks*. https://www.geeksforgeeks.org/append-to-json-file-using-python/?ref=lbp.\n",
    "\n",
    "* Bailey, Mark M. \"NRCLex.\" *Python Package Index (PyPI)*. 2019. https://pypi.org/project/NRCLex/. \n",
    "\n",
    "* Daityari, Shaumik. \"How To Perform Sentiment Analysis in Python 3 Using the Natural Language Toolkit (NLTK).\" *Digital Ocean*. 26 Sep 2019. https://www.digitalocean.com/community/tutorials/how-to-perform-sentiment-analysis-in-python-3-using-the-natural-language-toolkit-nltk.\n",
    "\n",
    "* \"Deserialize JSON to Object in Python.\" *Geeks for Geeks*. https://www.geeksforgeeks.org/deserialize-json-to-object-in-python/?ref=lbp.\n",
    "\n",
    "* \"JavaScript JSON.\" *Geeks for Geeks*. https://www.geeksforgeeks.org/javascript-json/.\n",
    "\n",
    "* \"JSON|Data Types.\" *Geeks for Geeks*. https://www.geeksforgeeks.org/json-data-types/.\n",
    "\n",
    "* \"json.load() in Python.\" *Geeks for Geeks*. https://www.geeksforgeeks.org/json-load-in-python/. \n",
    "\n",
    "* \"json.loads() in Python.\" *Geeks for Geeks*. https://www.geeksforgeeks.org/json-loads-in-python/.\n",
    "\n",
    "* \"json.dumps() in Python.\" *Geeks for Geeks*. https://www.geeksforgeeks.org/json-dumps-in-python/. \n",
    "\n",
    "* \"json.dump() in Python.\" *Geeks for Geeks*. https://www.geeksforgeeks.org/json-dump-in-python/.\n",
    "\n",
    "* \"Python JSON.\" *Geeks for Geeks*. https://www.geeksforgeeks.org/python-json/?ref=lbp.\n",
    "\n",
    "* \"Read, Write and Parse JSON using Python.\" *Geeks for Geeks*. https://www.geeksforgeeks.org/read-write-and-parse-json-using-python/?ref=lbp.\n",
    "\n",
    "* \"Serializing JSON data in Python.\" *Geeks for Geeks*. https://www.geeksforgeeks.org/serializing-json-data-in-python/?ref=lbp.\n",
    "\n",
    "* Summers, Ed, and Nick Ruest. \"Twarc.\" *GitHub*, https://github.com/DocNow/twarc/blob/4bff06bb8f2e279ea0a2064d6bfaec3c9b4b3aff/docs/api/library.md\n",
    "\n",
    "* Summers, Ed, and Nick Ruest. \"twarc.\" *twarc*. https://twarc-project.readthedocs.io/en/latest/twarc2_en_us/.\n",
    "\n",
    "* Twitter Developer Relations. \"getting-started-with-the-twitter-api-v2-for-academic-research.\" *GitHub*, https://github.com/twitterdev/getting-started-with-the-twitter-api-v2-for-academic-research/blob/f3b5b40d197ecaac133299eea46fefcced569d21/modules/6b-labs-code-standard-python.md\n",
    "\n",
    "* \"Working With JSON Data in Python.\" *Geeks for Geeks*. https://www.geeksforgeeks.org/working-with-json-data-in-python/?ref=lbp.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Copyright © 2022 WorldQuant University. This\n",
    "content is licensed solely for personal use. Redistribution or\n",
    "publication of this material is strictly prohibited.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Financial Data_Module 6_Lesson 1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
